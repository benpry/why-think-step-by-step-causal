"""
This file evaluates the samples generated by the local joint model
"""
import re
import torch
import numpy as np
import pandas as pd
from itertools import product
from argparse import ArgumentParser
from tqdm import tqdm
import pickle
import sys
from pyprojroot import here
from transformers import LogitsProcessor, LogitsProcessorList
sys.path.extend(["../core", "./code/core"])
from utils import (
    set_up_transformer,
    get_probability_from_transformer,
    PAD_TOKEN_ID,
    EQUALS_TOKEN_ID,
)
from networkx.exception import NetworkXError

# re-implemneted from newer version of HuggingFace (to avoid version conflict hell)
class SuppressTokensLogitsProcessor(LogitsProcessor):
        r"""This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so that they are not sampled."""
            
        def __init__(self, suppress_tokens):
            self.suppress_tokens = list(suppress_tokens)
                
        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
            scores[:, self.suppress_tokens] = -float("inf")
            return scores



def tokenize_vars(all_vars, tokenizer, args):
    var_tokens = {}
    for var in all_vars:
        tokens = tokenizer(var, return_tensors="pt").input_ids.to(args.device)
        var_tokens[var] = tokens.squeeze()

    return var_tokens


def generate_next_var(prefix, tokenizer, model, args, var_tokens, token_suppressor):
    # generate up to the next equals sign
    token_ids = tokenizer(prefix, return_tensors="pt").input_ids.to(args.device)
    outputs = model.generate(
        token_ids,
        return_dict_in_generate=True,
        output_scores=True,
        max_new_tokens=6,
        pad_token_id=PAD_TOKEN_ID,
        eos_token_id=EQUALS_TOKEN_ID,
        return_tensors="pt",
        logits_processor=token_suppressor
    )

    added_tokens = outputs.sequences[0][token_ids.shape[1]:]
    added_str = tokenizer.decode(added_tokens)

    return added_str


def main(args):
    model, tokenizer = set_up_transformer(args.model_folder, device=args.device)
    nets = pickle.load(open(here(args.bayes_net_file), "rb"))
    net = nets[args.net_idx]
    net.from_pickle()
    all_vars = net.graph.nodes
    var_tokens = tokenize_vars(all_vars, tokenizer, args)

    token_suppressor = LogitsProcessorList(
            [
                SuppressTokensLogitsProcessor(
                    suppress_tokens=tokenizer(["### target:"]).input_ids[0]
                    )
                 ]
    )

    df_pairs = pd.read_csv(
        here(f"data/training-data/selected-pairs/selected-pairs-net-{args.net_idx}.csv")
    )
    chosen_pairs = set()
    for index, row in df_pairs.iterrows():
        chosen_pairs.add((row["var1"], row["var2"]))
        chosen_pairs.add((row["var2"], row["var1"]))

    chosen_pairs = set(list(chosen_pairs))

    rows = []
    for target_var, condition_var in tqdm(chosen_pairs):
        if target_var == condition_var:
            continue

        for condition_val in (0, 1):
            for intervention in (False, True):
                target_probs, ns_intermediate, all_intermediate_vars, is_d_separating = (
                    [],
                    [],
                    set(),
                    [],
                )
                for sample in range(args.num_samples):
                    prefix = f"###\ntarget: {target_var}\n"
                    if intervention:
                        prefix += f"do({condition_var}={condition_val})\n"
                    else:
                        prefix += f"{condition_var}={condition_val}\n"

                    generated_var = None
                    target_prob = np.NaN
                    intermediate_vars = set()
                    for i in range(len(all_vars)):

                        next_str = generate_next_var(
                            prefix, tokenizer, model, args, var_tokens,
                            token_suppressor = token_suppressor
                        )

                        prefix += next_str
                        next_var = next_str.replace("=", "")
                        if target_var in next_var:
                            # get the target probability and break if we've generated
                            # the target variable
                            target_prob = get_probability_from_transformer(
                                prefix, tokenizer, model, args
                            )
                            break
                        elif next_var != condition_var:
                            intermediate_vars.add(next_var)

                        # get the next probability and sample a value
                        prob = get_probability_from_transformer(
                            prefix, tokenizer, model, args
                        )
                        val = np.random.choice(2, p=[1 - prob, prob])
                        prefix += str(val)
                        if "do" in next_str:
                            print(f"added do!")
                            prefix += ")"
                            print(prefix)
                        
                        prefix += "\n"

                    if not np.isnan(target_prob):
                        target_probs.append(target_prob)
                        ns_intermediate.append(len(intermediate_vars))
                        try:
                            is_d_separating.append(
                                int(
                                    net.check_d_separated(
                                        condition_var, target_var, intermediate_vars
                                    )
                                )
                            )
                            all_intermediate_vars |= intermediate_vars
                        except NetworkXError:
                            print(f"generated invalid variable name")

                rows.append(
                    {
                        "target_var": target_var,
                        "condition_var": condition_var,
                        "condition_val": condition_val,
                        "intervention": int(intervention),
                        "prob": np.mean(target_probs),
                        "prob_std": np.std(target_probs),
                        "prop_valid": len(target_probs) / args.num_samples,
                        "n_intermediate": np.mean(ns_intermediate),
                        "prop_d_separating": np.mean(is_d_separating),
                        "all_intermediate_vars": list(all_intermediate_vars),
                    }
                )

    df_free = pd.DataFrame(rows)
    return df_free


parser = ArgumentParser()
parser.add_argument("--model_folder", type=str)
parser.add_argument("--device", type=str, default="cpu")
parser.add_argument("--net_idx", type=int)
parser.add_argument("--bayes-net-file", type=str)
parser.add_argument("--num_samples", type=int, default=10)
parser.add_argument("--base_model_name", type=str)

if __name__ == "__main__":
    args = parser.parse_args()

    df_free = main(args)

    model_name = args.model_folder.split("/")[-1]
    df_free.to_csv(
        here(
            f"data/evaluation/causal/base-model-{args.base_model_name}/free-gen-probabilities-{model_name}-{args.num_samples}samples.csv"
        )
    )
